# awesome_segmentation
Methods of semantic segmentation with links to implementations and papers

## Models

Model | arXiv | Implementations
------------ | ------------- |  -------------
U-Net | https://arxiv.org/abs/1505.04597 <br> There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available. | Caffe (original): https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/ <br> Keras: https://github.com/yihui-he/u-net <br> TF: https://github.com/jakeret/tf_unet <br> PyTorch: https://github.com/jakeoung/Unet_pytorch <br> PyTorch https://github.com/bodokaiser/piwise
ENet | https://arxiv.org/abs/1606.02147 <br> The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18× faster, requires 75× less FLOPs, has 79× less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.| Caffe: https://github.com/TimoSaemann/ENet <br> LuaTorch: https://github.com/e-lab/ENet-training <br> Keras: https://github.com/PavlosMelissinos/enet-keras 
V-Net | https://arxiv.org/abs/1606.04797 <br> Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.| Caffe (original): https://github.com/faustomilletari/VNet <br> PyTorch: https://github.com/mattmacy/vnet.pytorch


## Datasets


Dataset | Description | Link | Convenient tools
------------ | ------------- | ------------- |  -------------
